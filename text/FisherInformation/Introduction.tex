To introduce the Fisher information (FI), we will start off with how it's defined and used in statistics.\\
We will consider a \textbf{statistical model} $f(x_i|\theta)$ that represents how a parameter $\theta$ is related to the outcomes $x_i$ of random variables $X_i$ \cite{StatisticFisherInfoTutorial}. Let's look at an example of a statistical model. You can see a picture of a Galton board in \cref{fig:GaltonPicture}.
\begin{figure}
	\centering
	\includegraphics[width = 4cm]{text/FisherInformation/plots/GaltonBoard.jpg}
	\caption{Photograph of a Galton board, taken from \cite{GaltonBoardPicture}.}
	\label{fig:GaltonPicture}
\end{figure}
It's a famous mechanical model that visualizes binomial distributions, which are discrete approximations of the normal distribution. If we place many balls at the top of the board and let them fall to the bottom, the amount of balls that end up in each cell are distributed according to the binomial distribution \cite{GaltonBoardArticle}. In this case, $x_i$ could assume the slot number which a ball can fall into. The $i$ could label multiple throws into the board, but for now we'll assume that there is only one experiment $i$. To introduce a parameter that influences the distribution of the balls, let's say one can throw from different spots above the Galton board which we now control with the the value of $\theta$. For a known $\theta$, the resulting function of the statistical model represents the probability distribution $f(x_i|\theta) = p_\theta(x_i)$ for the probability of the different outcomes $x_i$. As a sidenote, if we instead fixed the value of $x_i$ and viewed the statistical model as a function of $\theta$ it would be called a likelihood function. A visual representation of the probability distributions for different $\theta$ can be seen in \cref{fig:GaltonDistributions}.
\begin{figure}
	\centering
	\includegraphics[width = \textwidth, clip, trim= 0cm 0cm 0cm 2.3cm]{text/FisherInformation/plots/GaltonDistributionsPlot.pdf}
	\caption{This figure shows the probability distributions for a Galton board with different drop-in positions. The slots where the ball can end up are labeled by the value of $x_i$.}
	\label{fig:GaltonDistributions}
\end{figure}\\
In general, the statistical models might be more complex, where $\theta$ contains several parameters, $x$ is an element of a mathematical space other than $\mathbb{R}$ and the index $i$ denotes various different experiments, all depending on the same parameter but having different possible outcomes and probability distributions.\\
What's of interest for the FI are cases, where the parameters are not known before conducting the experiment and have to be approximated by the different outcomes $x_i$.\\
Before we introduce the FI, let's look at an example from Ly et al. \cite{StatisticFisherInfoTutorial}. Let's consider a biased coin where we denote the probability of heads ($x_i = 1$) with $\theta$ and the probability of tails ($x_i = 0$) with $1-\theta$. We will now take a look at the outcome of $n$ tosses, represented by the variable $X^n$. For example, an observed result for $X^5$ could be $x^5 = (1,1,0,1,0)$. Let's consider another variable $Y$, observing the sum of the total head throws $y = \sum x^n$. In our example case of $x^5 = (1,1,0,1,0)$, this would result in a value of $y = 3$. The probability for this variable $y$ is distributed according to the binomial distribution $f(y|\theta) = \binom{n}{y}\theta^y (1-\theta)^{n-y}$ \cite{BookOnBinomialDistributions}. Here, the binomial coefficient $\binom{n}{y}$ represents the different combinations that result in the same value of $y$. This is needed because there are $2^n$ different possibilities for $x$, while there are only $n$ different possibilities for $y$.\\
If we now fix the outcome of $y$ and look at the conditional probability for the different $x^n$ that could have resulted in that $y$ value, we get $p(x|y,\theta) = 1/ \binom{n}{y}$. With $p(x|y,\theta)$ we denote the probability depending on $x$ for fixed $y$ and $\theta$. Although the probability of $y$ and $x$ both depend on $\theta$, the probability for $x$ when $y$ is fixed doesn't. After measuring $y$, there is no information about $\theta$ left in the measurement of $x$. This means that $y$ is fully descriptive of, or sufficient for the parameter $\theta$. Measuring $y$ results in the same amount of information about the parameter $\theta$ as measuring the whole observation $x$. To quantify how much information a certain function contains about the parameters $\theta$, Fisher introduced the \textbf{Fisher information}.\\
The Fisher information is defined as 
\begin{equation}\label{eq:FIDefinition}
	I_{X,ij}(\theta) = \underset{x\in X}{E} \left[\tAbl{}{\theta_i}\log f(x|\theta) \cdot \tAbl{}{\theta_j}\log f(x|\theta)\right],
\end{equation}
where we used the expectation $E$
\begin{equation}
	\underset{x\in X}{E} \left[A(x)\right] = 
	\begin{cases}
		\sum_{x\in X} \left(A(x) p(x)\right) &\text{if $X$ is discrete},\\
		\int_{x\in X} A(x) p(x) \mathrm{d}x &\text{if $X$ is continuous}.
	\end{cases}
\end{equation}
We will later use an alternative notation where we denote $\log f$ as $\ell$. As will be evident later, this notation does not interfere with the definition of $\ell$ in \cref{eq:Loss_longform}. Since the Fisher information is dependent on $\theta$, we can fix the value of $\theta$ during calculation, which makes $f(x|\theta)$ equal to the probability density $p_\theta(x)$.\\ 
For $n$ independent experiments $X^n$, where $f(x^n|\theta) = \prod_{i=1}^n f(x_i|\theta)$, one can split the FI into 
\begin{equation}\label{eq:FIforIndependentExperiments}
	I_{X^n,ij}(\theta) = \prod_{i=1}^n I_{X_i,ij}(\theta).
\end{equation}
A proof of this can be found in \cref{sec:ProofFIforIndependentExperiments}.\\
For our example, the FI yields $I_{X^n}(\theta) = I_{Y}(\theta) = n/(\theta(1-\theta))$ \cite{StatisticFisherInfoTutorial}. This means that there's as much information about the $\theta$ contained in the measurement of $Y$ as in the measurement of $X^n$, which coincides with $Y$ being a sufficient measurement for $\theta$. \\
To give another example of how the FI represents the information obtainable about a parameter from a measurement, let's consider the family of normal distributions
\begin{equation}
	\mathcal{N}(x|\mu,\sigma) = \frac{1}{\sqrt{2\pi\sigma^2}}\mathrm{e}^{-(x-\mu)^2/(2\sigma^2)}.
\end{equation} 
These will now act as our statistical model $p_\theta(x) = \mathcal{N}(x|\theta)$, where $\theta = \{\theta_1,\theta_2\} = \{\mu, \sigma\}$. An observation would consist of a resulting value $x\in \mathbb{R}$, with its probability distributed according to the statistical model. The FI from equation \cref{eq:FIDefinition} can be derived as 
\begin{equation}
	I(\mu,\sigma) = \frac{1}{\sigma^2}
	\begin{pmatrix}
		1 & 0 \\
		0 & 2
	\end{pmatrix}.
\end{equation}
We can now interpret the diagonal elements as measures of how much information a measurement contains about the corresponding parameter, and the off-diagonal elements as measurements of how similarly the model changes when varying the corresponding parameters. To give a specific example, let's look at the diagonal component corresponding to $\mu$, $I_{11}(\mu,\sigma) = 1/\sigma^2$. This value indicates that for smaller $\sigma$, random values drawn from the distribution contain more information about $\mu$ than samples drawn from distributions with larger $\sigma$. For a visual guide, consider \cref{fig:NormalDistributionExample}.
\begin{figure}
	\centering
	\includegraphics{"text/FisherInformation/plots/NormalDistributionPlot.pdf"}
	\caption{This figure shows two normal distributions centered around $\mu = 2$ with varying $\sigma$ parameters. It also shows four samples chosen randomly according to the distribution. It's visible that for the case of a smaller variance $\sigma$, the points tend to be closer to the center and also less spread apart, which makes the information about $\mu$ contained in a measurement larger for a smaller variance.}
	\label{fig:NormalDistributionExample}
\end{figure}
It can be seen that the smaller value of $\sigma$ results in a narrower spread of randomly drawn values, as indicated by the orange arrow. The values also tend to be closer to the mean value $\mu$ the smaller the variance $\sigma$ is. Therefore, if we had to predict the value of $\mu$ from knowing only a few drawn samples, it would be easier to use the values drawn from the distribution with the smaller variance. This is because the information contained in the samples measured by the FI is greater there. Keep in mind that all of these drawn values are randomly distributed. Therefore it's also possible to have two sets of random samples where the samples from the larger variance are better at predicting $\mu$, but statistically speaking the smaller variance tends to perform better.

To conclude this chapter, the Fisher information is used in statistics to measure the amount of information one can gather about a parameter $\theta$ by measuring the outcome of a probability distribution $p_\theta(x_i)$. It is defined in \cref{eq:FIDefinition}.
