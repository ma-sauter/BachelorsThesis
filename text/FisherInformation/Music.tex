In the previous sections we've introduced Fisher Information, first in a statistical context and later as the metric of a statistical manifold, in order to apply some of its insights to neural network training.\\
Since the information provided in the last sections has been mathematically abstract and lacking intuition, let's quickly recap the basics of what we need to know and understand about the Fisher Information for the rest of this work.\\
First, let's state the definition again. The FI is defined as 
\begin{equation}
	\begin{split}
		I_{ij} &= \underset{x \in X}{E} \left[\tAbl{}{\theta_i}\log f(x|\theta)\cdot \tAbl{}{\theta_j}\log f(x|\theta)\right]\\
		&= \underset{(\mathbf{x}_i,\mathbf{y}_i)\in D}{E} \left[\tAbl{}{\theta_i}\ell_\theta(\mathbf{x}_i,\mathbf{y}_i)\cdot \tAbl{}{\theta_j}\ell_\theta(\mathbf{x}_i,\mathbf{y}_i)\right],
	\end{split}
\end{equation}
where the notation in the first line is used in the context of statistics and the one from the second line in the context of the manifold of neural network training.\\
We first introduced the FI as a statistical measure of how much information the measurement of a random variable contains about its underlying parameters.\\
For example, let's say our experiment consists of throwing a biased coin, where the probability of heads is the underlying parameter. We can conduct an experiment to estimate the probability of the biased coin. Let's say we make 10 throws twice. The first time, we will use the full observation for the parameter estimation. We will exactly know the results of the coin toss \emph{for every single one} of the 10 throws. The second time, we will only know how often heads came up \emph{in total} for the 10 throws. We will disregard information about when exactly during these 10 trials we measured heads or tails. The Fisher Information with respect to the parameter yields the same value for both of these observables. This therefore means that the information about the probability of heads contained in the measurement is the same for both observations \cite{StatisticFisherInfoTutorial}. To estimate the bias of the coin, it is perfectly sufficient to only write down the total number of heads instead of the entire observation.\\
Going further, we introduced another application of the FI as the Riemannian metric of the statistical manifold corresponding to a neural network. For this, we considered a neural network along with a dataset and loss function as a statistical model. We did this by introducing a kind of probability distribution $p_\theta(\mathbf{x}_i,\mathbf{y}_i) = \mathrm{e}^{-\ell_\theta(\mathbf{x}_i,\mathbf{y}_i)}$. This probability measures if our network with parameters $\theta$ produces the correct output $\mathbf{y}_i$ when given the input $\mathbf{x}_i$. The family of probability distributions $\{p_\theta\}$ forms a statistical manifold \cite{AmarisLectureNotes}. It is a topological space with coordinates $\theta = \{\theta_1, \ldots, \theta_n\}$, where every point in the space corresponds to a probability function that depends on the $\theta$ coordinates. Locally, it is isometric to a subset of $\mathbb{R}^n$. Globally, euclidean geometry doesn't apply, which for example makes the shortest paths between points in the manifold curves in the coordinate system. Because every point in the space is a probability function, it's very hard to properly imagine lines between points and even the seemingly simple concept of distance becomes very abstract. Therefore, our next goal was defining how one can measure distance and characterize curvature in this space.\\
To be able to define what distance means, we first introduced the concept of a tangent spaces. A tangent space is, again hard to grasp, an abstract vector space defined at a point in the manifold, where the vectors are differential operators corresponding to the derivatives along curves through the point \cite{AmarisLectureNotes}. The reason we defined it is because by defining an inner product $\langle \cdot , \cdot \rangle$ on the tangent spaces, we can obtain a metric on the manifold through $g_{ij} = \langle \mathbf{e}_i,\mathbf{e}_j\rangle$ \cite{AmarisLectureNotes}. The basis vectors $\mathbf{e}_i$ on the tangent spaces are abstract concepts, which makes finding an intuitive definition of an inner product non trivial. That's why we introduced a new space, isomorph to the tangent space, where defining the inner product feels natural. The ismorphism was introduced as \cite{AmarisLectureNotes}
\begin{equation}
	\partial_i \leftrightarrow \partial_i \ell(x|\theta)
\end{equation}
and maps the derivative operators onto actual derivatives of the logarithm of the statistical model $f$. To obtain the inner product of two derivative operators we then defined \cite{AmarisLectureNotes}
\begin{equation}
	\langle \partial_i, \partial_j \rangle = \langle \partial_i \ell(x|\theta), \partial_j \ell(x|\theta) \rangle,
\end{equation}
where we naturally assume the inner product between the two distributions as
\begin{equation}
	 \langle \partial_i \ell_\theta(\mathbf{x}_i,\mathbf{y}_i), \partial_j \ell_\theta(\mathbf{x}_i,\mathbf{y}_i) \rangle = \underset{(\mathbf{x}_i,\mathbf{y}_i)\in D}{E} \big[\partial_i \ell_\theta(\mathbf{x}_i,\mathbf{y}_i) \cdot \partial_j \ell_\theta(\mathbf{x}_i,\mathbf{y}_i)\big].
\end{equation}
Now we arrived at the definition of the metric of our manifold, which is equivalent to the Fisher Information matrix
\begin{equation}\label{eq:MetricDefinitionInInterpretationChapter}
	I_{ij} = g_{ij} = \underset{(\mathbf{x}_i,\mathbf{y}_i)\in D}{E} \big[\partial_i \ell_\theta(\mathbf{x}_i,\mathbf{y}_i) \cdot \partial_j \ell_\theta(\mathbf{x}_i,\mathbf{y}_i)\big].
\end{equation}
Since this equation is a bit easier to grasp, we can finally try to get some intuition about the connections of the concepts mentioned before.\\
Distance between two points in a curved manifold along a curve $c$ is defined as \cite{AmarisLectureNotes}
\begin{equation}
	s = \int_{t_0}^{t_1} \sum_{i,j} \sqrt{g_{ij}\tAbl{\theta_i}{t}\tAbl{\theta_j}{t}} \mathrm{d}t. 
\end{equation}
If we only move along one coordinate $\theta_i$ on the curve, the distance reduces to 
\begin{equation}
	s = \int_{t_0}^{t_1} \sqrt{g_{ii}}\left|\tAbl{\theta_i}{t} \right| \mathrm{d}t. 
\end{equation}
Therefore the diagonal components of the FI tell us how far we move across the manifold when we change parameter $\theta_i$. Through inspection of the equation for the metric (\cref{eq:MetricDefinitionInInterpretationChapter}), it's clear that the distance between points created by changing a parameter relates to the expected change in the logarithm of the probability distribution. We can directly map the parameter space onto the manifold of probabilities, but we need to reconsider the notion of distance for the manifold because distance in the euclidean parameter space doesn't generally translate to distance in the manifold, which is a measure of difference between the probabilities.\\
Finally let's examine the components of the metric for the neural network. The diagonal components are the expectation values of the squared derivative of the loss. This means that the diagonal values represent how much our subloss changes when we vary the corresponding parameter. The off-diagonal values represent how similar the change in the subloss function is under changes in the two corresponding parameters. This information about the change of the loss regarding the parameters is now of high interest when considering neural network training. In general, calculating the FI computationally is very costly though, because the number of parameters can be very high for complex tasks. That's why the results of this work consider a few observations resulting from the fisher matrix and investigate some possibilities to obtain them from other, computationally easier, observables of training instead.