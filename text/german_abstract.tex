Deep Learning hat sich als erfolgreiche Methode zur Lösung verschiedenster komplexer Problemstellungen erwiesen. Dies ist auf den bemerkenswert einfachen Aufbau seiner neuronalen Netze zurückzuführen, der in starkem Kontrast zu der Komplexität der reproduzierbaren Funktionen steht. Allerdings ist das Verständnis der genauen Konfigurationen, die den neuronalen Netzen das Lösen der komplexen Probleme ermöglichen, nur sehr gering. Neue Erkenntnisse über die Geometrie hinter der Loss-Funktion und die Architektur der Netzwerke könnten die Entwicklung neuer Methoden auf dem Gebiet des maschinellen Lernens deutlich vorantreiben.\\
Diese Arbeit beschäftigt sich mit zwei mathematischen Grö\ss en, die tiefere Einsichten in die Mechaniken von neuronalen Netzen und ihrem Training versprechen: Der Fisher information und dem Neural Tangent Kernel. Zunächst werden diese mathematisch hergeleitet und ihr Einfluss auf die neuronalen Netze erläutert. Weiterhin wird beschrieben, wie die skalare Ricci Krümmung aus der Fisher information berechnet werden kann. Darauf aufbauend wird eine neue Relation zwischen der Fisher information und dem NTK hergeleitet und diese anhand eines Beispieltrainings für den MNIST Datensatz untersucht. Des Weiteren werden alle zuvor genannten Grö\ss en für das Beispiel eines einfachen Netzwerkes berechnet und die erzielten Ergebnisse analysiert.