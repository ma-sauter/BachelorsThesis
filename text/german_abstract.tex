Deep learning hat sich erfolgreich als Methode zur Lösung verschiedenster komplexer Problemstellungen bewährt. Dies ist auf den bemerkenswert einfachen Aufbau seiner neuronalen Netze zurückzuführen, der in starkem Kontrast zu der Komplexität der reproduzierbaren Funktionen steht. Allerdings ist unser Verständnis der genauen Konfigurationen, die den neuronalen Netzen das Lösen der komplexen Probleme ermöglichen, nur sehr gering. Neue Erkenntnisse über die Geometrie hinter der loss Funktion und die Architektur der Netzwerke könnten die Entwicklung neuer Methoden auf dem Gebiet des maschinellen Lernens deutlich vorantreiben.\\
Im Verlauf dieser Arbeit werden wir die Fisher Information und den NTK untersuchen. Diese sind zwei mathematische Grö\ss en, die tiefere Einsichten in die Mechaniken von neuronalen Netzen und ihrem Training versprechen. Zunächst werden wir diese mathematisch herleiten und ihren Einfluss auf die neuronalen Netze erläutern. Wir werden weiterhin erläutern, wie die skalare Ricci Krümmung aus der Fisher Information berechnet werden kann. Darauf aufbauend werden wir eine neue Relation zwischen der Fisher Information und dem NTK herleiten und diese Anhand eines Beispieltrainings für den MNIST Datensatz untersuchen. Des Weiteren werden wir alle zuvor genannten Grö\ss en für das Beispiel eines simplen Netzwerkes messen und die erzielten Ergebnisse analysieren.