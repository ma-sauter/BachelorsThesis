\subsubsection{Neurons}
The term "neural network" was originally used to describe the workings of nervous systems of animals. These systems consist of a net of neurons that are intricately connected by synapses. These connections carry electrical pulses between the different neurons that can excite them if they exceed certain thresholds, which vary from neuron to neuron and change over time. For example the excitability immediately after activation is almost zero. When this excitation occurs, new pulses in turn propagate through the connections of the excited neuron. Mathematical analyses of these systems have been done as early as the 1940's.\cite{A_logical_calculus_of_the_ideas_immanent_in_nervous_activity}\\
The artificial neural networks that we use for machine learning are inspired by those biological networks. The neurons in our case are mathematical entities that take a fixed number of scalar values as inputs and convert them into a single output value. For a more visual explanation let's take a look at \cref{fig:Neuron_explanation} for the simple case of a 2-input neuron. 
\begin{figure}
	\centering
	\input{text/MachineLearningBasics/plots/neuron_plot.tex}
	\caption{This figure explains the general operating principle of neurons in neural networks as they are used in this thesis. The parameters are denoted in orange, the input activations in blue and the activation with its corresponding activation function in green.}
	\label{fig:Neuron_explanation}
\end{figure}
The big circle in the middle is representing what we consider a neuron. It takes as input the activation values $a_i$, multiplies them with their corresponding weights $\omega_i$, sums them up, adds a bias value $b$ and then applies the activation function to get the resulting output value. The input activations $a_i$ correspond to the electronic pulses in the nerve system, with the strength of the pulse coded into the value itself. No pulse in the biological system would be represented by an activation of zero in the mathematical model. The weights $\omega_i$ are a representation of how important single input values are for the activation of the neuron itself. In the biological counterpart this might correspond to how thick or conductive the connections between the nerve cells are. Finally, the combination of bias $b$ and activation function describes how high the sum of the input-weight-pairs has to be to activate the neuron and how the resulting value for the activation of the neuron changes for higher input activations. For example, a very simple output activation function would be a heaviside function. The neuron then outputs $1$ if the sum of the input-weight-pairs is bigger than the negative bias $-b$ and a 0 otherwise. Hence the neuron can only be on or off. A more common activation function that will be used for the rest of this thesis is the ReLU function (Rectified Linear Unit). This function is defined as 
\begin{equation}
	f(x) = 
	\begin{cases}
		x, &\text{if } x\geq0 \\
		0, &\text{otherwise}
	\end{cases}.
\end{equation}
Here the neuron gets activated as soon as the sum of the input-weight-pairs is higher than $-b$ and the value of the activation increases linearly with the weighted input summation.
\subsubsection{Neural networks}
A neural network can be built from these neurons, by connecting the outputs of certain neurons to the inputs of others. For example lets consider a neural network that consists of 3 layers of 4 neurons each, that takes 2 values as an input and outputs 2 values at the end. This could for example be trained for detecting if a point on a 2d-grid is inside or outside of a given region. The input values would be the $x$ and $y$ coordinates of the point and the output value could represent the probability of the point being inside the desired region. How exactly this training would work will be explained in later chapters. A visual sketch of such a network can be seen in \cref{fig:Neural_network_example}. \\
\begin{figure}
	\centering
	\input{text/MachineLearningBasics/plots/neural_network_plot.tex}
	\caption{This figure shows an example of a neural network. It consists of multiple neurons connected to each other. This specific network has 2 input values and one output value. The names of those input and output values correspond to the use case of identifying if a 2d point lies within a region in 2d space.}
	\label{fig:Neural_network_example}
\end{figure}
This network is only a very specific example of what a neural network might look like. In reality, there are various kinds of networks used to learn different tasks. For this thesis we will only talk about "fully connected" or "dense" neural networks. This means that every neuron in the first layer will receive every possible input value, and every neuron in later layers will receive the output of every neuron in the previous layer as an input. How the output gets handled may still differ through different use cases. The structure of a neural network is generally called the architecture of the network. 