In the previous sections we took a brief look at machine learning and neural networks. Although this is still barely scratching the surface of the methods that are used today, it is still more than what's needed for the upcoming chapters.\\
It is sufficient for all mathematical considerations that follow, to view systems $f_\theta(\mathbf{x}_i)$ that depend on parameters $\theta$, accept input vectors $\mathbf{x}_i$ of constant dimension and can be evaluated through the formalism of the loss function from equation \cref{eq:Loss_longform}. For the discussions of the NTK, we can even disregard the assumption of the loss function splitting up into a sum of $\ell$ functions. This means that the exact properties of the neural networks we looked at earlier can be disregarded, which means we can generalize the observations to many more network architectures or completely different systems than previously mentioned.