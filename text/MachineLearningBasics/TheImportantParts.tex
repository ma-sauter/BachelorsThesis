In the previous sections we took a brief look at machine learning and neural networks. Although this barely scratches the surface of the methods used nowadays, it's still more specific than what's needed for \cref{sec:ChapterFisherInformation} and \cref{sec:ChapterNTK}. The specific methods were given to explain how NNs in this thesis are trained, but include restrictions that don't need to be made for the mathematical considerations coming up.\\
For those chapters it is sufficient to view mathematical functions $f_\theta(\mathbf{x}_i)$ that depend on parameters $\theta$, accept input vectors $\mathbf{x}_i$ of fixed dimension and can be evaluated through a loss function of the same shape as \cref{eq:Loss_longform}. For the discussions of the NTK, we can even disregard the assumption of the loss function splitting up into a sum of subloss functions. This means that the exact properties of the neural networks we looked at earlier can be ignored, allowing us to generalize the observations to many more network architectures or completely different systems than previously mentioned.