In the previous sections we took a brief look at machine learning and neural networks. Although this barely scratches the surface of the methods that are used today, it's still more than what's needed for the upcoming chapters. The specific methods were given to explain how the NNs that will come up later in this thesis were trained, but are unnecessary restrictions that don't need to be made for the mathematical considerations coming up.\\
For those it is sufficient to view systems $f_\theta(\mathbf{x}_i)$ that depend on parameters $\theta$, accept input vectors $\mathbf{x}_i$ of constant dimension and can be evaluated through the formalism of the loss function from equation \cref{eq:Loss_longform}. For the discussions of the NTK, we can even disregard the assumption of the loss function splitting up into a sum of $\ell$ functions. This means that the exact properties of the neural networks we looked at earlier can be ignored, allowing us to generalize the observations to many more network architectures or completely different systems than previously mentioned.