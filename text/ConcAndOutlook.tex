This work mostly explained the mathematical foundation of Fisher Information, Neural Tangent Kernel and Ricci scalar curvature in their application for neural network training. We described the mathematical basics behind machine learning in \cref{sec:ChapterMachineLearningBasics}. \cref{sec:ChapterFisherInformation} dealt with the statistical background of the Fisher Information, how it represents the Riemannian metric of statistical manifolds, its application for neural networks and how it can be utilized to find phase transitions in physical systems. Furthermore, we stated how the scalar curvature of manifolds is defined and why we can derive it from the Fisher Information in the context of neural networks. In \cref{sec:ChapterNTK} we derived the NTK and noted which aspect of neural network training it describes.\\
\cref{sec:ChapterResults} started with the derivation of a new relation between the trace of the Fisher Information and the NTK in \cref{eq:FisherNTKRelation}. This relation is of great interest, since we can obtain information about the Fisher Trace from the NTK, which is a smaller and also easier to compute observable of neural network training. It also factorizes the Fisher Trace, which is a description of the size of the parameter updates in GD, into a component that only depends on the loss function and a component that depends only on the network architecture. To investigate whether the NTK is sufficient to describe the evolution of the Fisher Trace through this equation, we compared the two traces for an example training on the MNIST dataset in \cref{sec:TraceComparisonExperiment}. The result indicated that for some combinations of loss function and network, the behavior of the Fisher Trace can be approximated by the NTK Trace, but not in general. Further investigations are needed to determine the characteristics of network and loss function, for which the traces behave similarly.\\
In \cref{sec:ResultsOf2ParameterNetwork} we calculated the full Fisher Information, the NTK Trace and the scalar curvature for a simple two-input neural network. The results obtained are a first step towards understanding how these observables describe neural network training.\\
Overall, this work investigated the Fisher Information, the NTK Trace and the scalar curvature of neural networks. It provided mathematical backgrounds and first calculations of these observables for simple problems. Further research possibilities are vast. For example, there is already ongoing research attempting to make its computation feasible \cite{EfficientFisherResearch}. The Fisher Information could be used for training optimization, as in the Natural Gradient Descent algorithm proposed by Amari in \cite{NGDWorksEfficiently}, but one could also investigate its use for finding efficient initial states or even look for analogies of physical phase transitions in neural networks as described in \cref{sec:FIPhysics}. Regarding the NTK, one could also investigate the possibility of using it for training optimization in similar ways. For the scalar curvature, further research should delve deeper into its theoretical implications, to reveal insight into the specific information it contains. 
