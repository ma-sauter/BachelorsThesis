This work mostly dealt with the mathematical foundation of Fisher Information, Neural Tangent Kernel and Ricci scalar curvature in their application for neural network training. We described the mathematical basics behind machine learning in \cref{sec:ChapterMachineLearningBasics}. \cref{sec:ChapterFisherInformation} dealt with the statistical background of the Fisher Information, how it represents the Riemannian metric of statistical manifolds, its application for neural networks and how it can be utilized to find phase transitions in physical systems. Further, we stated how the scalar curvature of manifolds is defined and why we can derive it from the Fisher Information in the context of neural networks. In \cref{sec:ChapterNTK} we derived the NTK and noted what aspect of neural network training it describes.\\
\cref{sec:ChapterResults} started with the derivation of a new relation between the trace of the Fisher Information and the NTK in \cref{eq:FisherNTKRelation}. This relation is of great interest, since we can obtain information about the Fisher Trace from the NTK, which is a way smaller and therefore easier to compute observable of neural network training. It also splits up the Fisher Trace, which is a description of the size of the parameter updates in GD, into a part that depends only on the loss function and a part that depends only on the network architecture, represented by the NTK. To investigate if the NTK is sufficient for describing the evolution of the Fisher Trace through this equation, we compared the two traces for an example training on the MNIST dataset in \cref{sec:TraceComparisonExperiment}. The result indicated, that for some combinations of loss function and network, the behavior of the Fisher Trace can be approximated by the NTK Trace, but not in general. Further investigations are needed in search of the characteristics of network and loss function, for which the traces behave similarly.\\
In \cref{sec:ResultsOf2ParameterNetwork} we investigated the behavior of Fisher Information, NTK Trace and scalar curvature for a simple two-input neural network. The results obtained are a first step towards understanding how these observables describe neural network training, but the actual information contained is not yet clear.\\
Overall, this work investigated the Fisher Information, the NTK Trace and the scalar curvature of neural networks. It provided mathematical backgrounds and first calculations of these observables for simple problems. Further research is needed to determine what exact story they tell and how they could eventually be used to optimize training, find optimal initial states or determine the optimal architecture of neural networks.
