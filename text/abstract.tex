Deep learning has proven to be a powerful approach for tackling computationally intricate tasks (\cite{DeepLearning:DNA}, \cite{DeepLearning:DrugRecognition}, \cite{DeepLearning:ImageNetClassification}, \cite{DeepLearning:Translation}). Its success can be attributed to the remarkably simple nature of the models and optimization techniques it employs, especially when contrasted with the complexity of the tasks it can accomplish. Yet, our understanding of how these simple models adapt to meet complex requirements remains limited. Substantial improvements in the development of better models could be gained from understanding how the geometry of the loss function and the architecture of the network affects the training dynamics.\\
In this work, we investigate the Fisher Information and the Neural Tangent Kernel, two observables of neural network training that promise deeper insights into the workings behind neural learning. We derive both of them mathematically and explain their impact on the neural network and its training. Additionally, we mention how to compute the Ricci scalar curvature of a network from the Fisher Information. Building on these foundations, we present a novel relation between the trace of the Fisher Information and the NTK and investigate its applicability through an example training on the MNIST dataset. Moreover, we provide an example measurement of the complete Fisher Information, the trace of the NTK and the scalar curvature for a simple 2-parameter Network.