The NTK is defined as 
\begin{equation}\label{eq:NTKDefinition}
	\Lambda_{i,j,\alpha,\beta} = \nabla_\theta f_\theta(\mathbf{x}_i)_\alpha \cdot \nabla_\theta f_\theta(\mathbf{x}_j)_\beta.
\end{equation}
For now, let's assume that the neural network has only one output, which gets rid of the $\alpha$ and $\beta$ indices for us and makes the NTK a regular matrix. We can use this matrix to calculate the "time" derivative of the neural network output by 
\begin{equation}\label{eq:NTKExplanation}
	\pAbl{}{t}f_{\theta(t)}(\mathbf{x}_i) = \sum_j \Lambda_{i,j} \left(- \pAbl{\mathscr{L}}{f_{\theta(t)}(\mathbf{x}_j)}\right).
\end{equation}
this means that the evolution of the network output for input $\mathbf{x}_i$ is influenced by the outputs for other input values through the NTK. We can investigate this further by taking a look at the definition of the NTK above in \cref{eq:NTKDefinition}. \\

A mathematical kernel $K$ is defined as a function
\begin{equation}
	K(\mathbf{x}_i, \mathbf{x}_j) = \phi(\mathbf{x}_i)\cdot\phi(\mathbf{x}_i),
\end{equation}
with another so-called "feature map" function $\phi$ that maps the points $\mathbf{x}_i$ into a higher dimensional inner product space called the "feature space". The kernel assigns a scalar value to the two points by comparing their "features" via a scalar product in the feature space. In our case the feature map is $\nabla_\theta$ which maps our scalar network output onto a vector that contains the derivatives of the network output with respect to all of the various parameters. This is our feature space because what we're interested in is how moving in $\theta$ space changes the output values of our network. If we compare those two mappings we get a value that measures how closely the direction that increases $f_\theta(\mathbf{x}_i)$ in the most effective way (which is the direction the gradient points to) matches with the direction that increases $f_\theta(\mathbf{x}_j)$ most effectively. This also means that $\Lambda_{i,j}=0$ means that varying $\theta$ in the direction that changes $f_\theta(\mathbf{x}_i)$ most effectively results in 0 change for $f_\theta(\mathbf{x}_i)$.\\
Coming back to \cref{eq:NTKExplanation} the right side is the negative loss derivative with respect to $f_\theta(\mathbf{x}_j)$. Since we update the parameters in a way that minimizes the loss most effectively in gradient flow, the negative loss derivative with respect to $f_\theta(\mathbf{x}_j)$ describes how beneficial increasing $f_\theta(\mathbf{x}_j)$ is for our system. If we now multiply this with the NTK, which is a kernel that describes how similar $f_\theta(\mathbf{x}_i)$ and $f_\theta(\mathbf{x}_j)$ behave when changing theta, and sum everything up, we get the change of the function value $f_\theta(\mathbf{x}_i)$ as result.\\
In $\theta$ space, we can directly relate the evolution of $\theta$ to its negative loss derivative, because the parameters are what we are changing. For the derivative of the output values, we need the NTK as well, because we don't change the function value directly. If the loss derivative with respect to a output value tells the system that it should increase this output value, the system changes parameters in a way that increases this output value. The difference to the derivative of $\theta$ is that the change of parameters now doesn't reflect in just one output, but in every other output value as well. That's where the NTK arises in the equation. The NTK is in a way a translation of the SGD into the space of the outputs of the neural network.\\
All of the explanations above apply to the 4 dimensional hypermatrix form of the NTK for multidimensional neural network output as well, which can be seen when comparing \cref{eq:NTKExplanation} to \cref{eq:NTKarisesFromHere}.