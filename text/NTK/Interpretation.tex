Through \cref{eq:NTKarisesFromHere}, the NTK is defined as 
\begin{equation}\label{eq:NTKDefinition}
	\Lambda_{ij\alpha\beta} = \nabla_\theta f_\theta(\mathbf{x}_i)_\alpha \cdot \nabla_\theta f_\theta(\mathbf{x}_j)_\beta.
\end{equation}
For this explanation, let's assume that the neural network has only one output $f_\theta(\mathbf{x}_i)_\alpha \rightarrow f_\theta(\mathbf{x}_i)$, which gets rid of the $\alpha$ and $\beta$ indices for us and makes the NTK a regular matrix. We can use this matrix to calculate the time derivative of the neural network output similar to \cref{eq:NTKarisesFromHere} by 
\begin{equation}\label{eq:NTKExplanation}
	\tAbl{}{t}f_{\theta(t)}(\mathbf{x}_i) = \sum_j \Lambda_{ij} \left(- \pAbl{\mathscr{L}}{f_{\theta(t)}(\mathbf{x}_j)}\right).
\end{equation}
This means that the evolution of the network output for input $\mathbf{x}_i$ is influenced by the outputs for other input values $\mathbf{x}_j$ through the NTK. We can investigate this further by taking a look at the definition of the NTK above in \cref{eq:NTKDefinition}. \\
A mathematical kernel $K$ is defined as a function \cite{KernelMethod}
\begin{equation}
	K(\mathbf{x}_i, \mathbf{x}_j) = \phi(\mathbf{x}_i)\cdot\phi(\mathbf{x}_i),
\end{equation}
if it can be expressed using a so-called "feature map" function $\phi$, that maps the points $\mathbf{x}_i$ into a higher dimensional inner product space called the "feature space". The kernel $K$ assigns a scalar value to the two points by comparing their "features" via a scalar product in the feature space. For more specific information on Kernels and how they are used, please refer to \cite{KernelMethod}.\\
In our case the feature map is $\nabla_\theta$ which maps our scalar network output onto a vector that contains the derivatives of the network output with respect to all of the various parameters. This is our feature space, because what we're interested in is how moving in $\theta$ space changes the output values of our network. If we compare those two mappings we get a value that measures how closely the direction that increases $f_\theta(\mathbf{x}_i)$ most effectively matches with the direction that increases $f_\theta(\mathbf{x}_j)$ most effectively  (which are the directions the gradients point to). For example, $\Lambda_{ij}=0$ means that varying $\theta$ in the direction that changes $f_\theta(\mathbf{x}_i)$ most effectively results in 0 change for $f_\theta(\mathbf{x}_j)$.\\

Coming back to \cref{eq:NTKExplanation} the right side is the negative loss derivative with respect to $f_\theta(\mathbf{x}_j)$. Since we update the parameters in a way that minimizes the loss most effectively in gradient flow, the negative loss derivative with respect to $f_\theta(\mathbf{x}_j)$ describes how beneficial increasing $f_\theta(\mathbf{x}_j)$ is for our system. If we now multiply this with the NTK, which is a kernel that describes how similar $f_\theta(\mathbf{x}_i)$ and $f_\theta(\mathbf{x}_j)$ behave when changing theta, and sum everything up, we get the change of the function value $f_\theta(\mathbf{x}_i)$ as result.\\
In $\theta$ space, we can directly relate the evolution of $\theta$ to its negative loss derivative, because the parameters themselves are what we update with our algorithm. For the derivative of the output values we need the NTK as well, because we don't change the output values directly. If the loss derivative with respect to an output value tells the system that it \textit{should increase this output value}, the system \textit{changes the parameters} in a way that increases this output value. The difference to the derivative of $\theta$ is that the change of parameters now doesn't reflect in just one output, but in every other output value as well. That's where the NTK arises in the equation. The NTK is, in a way, a translation of the GD into the space of outputs of the neural network.\\
All of the explanations above apply to the 4 dimensional hypermatrix-form of the NTK for multidimensional neural network output as well, which can be seen when comparing \cref{eq:NTKExplanation} to \cref{eq:NTKarisesFromHere}.