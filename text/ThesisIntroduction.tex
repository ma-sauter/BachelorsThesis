In recent years, the widespread of deep learning methods lead to many advancements in the realm of computational problem solving. Many tasks that otherwise require vastly complex algorithms can also be solved by comparably simple neural networks. These networks act as function approximators that are optimized to fit certain tasks through simple methods such as gradient descent. The backside of this is that, while the networks architecture is remarkably simple, the specific configurations behind trained networks are exceptionally complex. The process of training and the resulting algorithm blackboxes are only poorly understood. That's why many current fields of research aim to find underlying concepts behind neural learning, in order to improve optimization algorithms and determine optimal network architectures for different tasks.\\
This thesis presents two different observables of neural network training, the Fisher Information and the NTK, that promise deeper insights into the training and architecture of neural networks. Its main goal is providing a deeper understanding of those observables, by first presenting their mathematical background along with interpretable explanations, and then investigating their relation and calculating them for a simple example setup.\\
To start off, \cref{sec:ChapterMachineLearningBasics} covers all fundamentals about machine learning needed for readers that are unfamiliar with the basic concepts behind neural networks and supervised learning.\\
\cref{sec:ChapterFisherInformation} discusses the mathematical foundation of the Fisher Information and some of its use cases. It explains how the Fisher Information represents the Riemannian metric of the statistical manifold constructed from the neural network and its loss function. Here, we also explain the concept of scalar curvature and how we can compute it from the Fisher Information for neural network training. %To explain the ideas behind \cref{sec:FisherInterpretation}, let's first visit a quote from Cristopher Nolan's movie "Oppenheimer": "Algebra is like sheet music. The important thing isn't can you read music, its can you hear it". This analogy beautifully explains how important it is not only to understand the precise meaning of mathematical equations, but also to get a deeper understanding and intuition about what information is contained inside of them. \cref{sec:FisherInformationAsRiemannianMetric(BigChapter)} provides a lot of complex and abstract mathematics, it only contains "sheet music" so to speak. It can be hard to decipher the meaning behind its abstract concepts. That's why \cref{sec:FisherInterpretation} provides actual "music", a summary and further explanation of the abstract math before, to convey a deeper understanding of the purpose behind the mathematics.
\\
\cref{sec:ChapterNTK} introduces the NTK, by going over a derivation and providing explanations on why it arises.\\
\cref{sec:ChapterResults} is divided into two sections. In \cref{sec:Results1}, we derive a relationship between the trace of the Fisher Information and the NTK and examine how accurately the behavior of their traces match up for a training example on the MNIST dataset. In \cref{sec:Results2}, we compute examples of the Fisher Information, the NTK Trace and the scalar curvature for a simple, 2-input neural network.\\
\cref{sec:ChapterConcAndOutlook} provides a summary of the mathematical concepts explained and investigated in this work. We also discuss potential further topics of research on the considered observables.