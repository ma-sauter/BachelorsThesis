This thesis presents two different observables of neural network training, the Fisher Information and the NTK. It's main goal is providing a deeper understanding of those observables, by first presenting their mathematical backround along with interpretable explanations, and then investigating their relation and calculating them for a simple example setup.\\
To start off, \cref{sec:ChapterMachineLearningBasics} covers all fundamentals about machine learning needed for readers that are unfamiliar with the basic concepts behind neural networks and supervised learning. It consists of \cref{sec:NeuralNetworks(BigSection)}, which covers what neural networks are and how they are described mathematically, \cref{sec:NeuralNetworkTraining}, which explains how neural networks can be used to replicate underlying relationships in datasets. \cref{sec:WhichAssumptionsAreNecessary} explains, which mathematical foundations mentioned before are actually necessary for the concepts of Fisher Information and NTK, since they are applicable to more general parameter dependent functions, outside of what was explained previously.\\
\cref{sec:ChapterFisherInformation} covers the mathematical foundation of the Fisher Information and some of its use cases. \cref{sec:FIinStatistics} first explains the original use of the Fisher Information in Statistics. It's used as an introduction to the Fisher Information, but can be disregarded for its application to neural networks. \cref{sec:FisherInformationAsRiemannianMetric(BigChapter)} deals with the concept of Riemannian manifolds and how the Fisher Information represents a Riemannian metric on statistical manifolds. The concepts explained here bridge the gap between the mathematical concept of the Fisher Information and how we can apply it to neural networks. Here we will also explain the concept of scalar curvature and how we can compute it from the Fisher Information for neural network training. %To explain the ideas behind \cref{sec:FisherInterpretation}, let's first visit a quote from Cristopher Nolan's movie "Oppenheimer": "Algebra is like sheet music. The important thing isn't can you read music, its can you hear it". This analogy beautifully explains how important it is not only to understand the precise meaning of mathematical equations, but also to get a deeper understanding and intuition about what information is contained inside of them. \cref{sec:FisherInformationAsRiemannianMetric(BigChapter)} provides a lot of complex and abstract mathematics, it only contains "sheet music" so to speak. It can be hard to decipher the meaning behind its abstract concepts. That's why \cref{sec:FisherInterpretation} provides actual "music", a summary and further explanation of the abstract math before, to convey a deeper understanding of the purpose behind the mathematics.
\cref{sec:FisherInterpretation} provides a summary and further explanations of the abstract math in \cref{sec:FisherInformationAsRiemannianMetric(BigChapter)}, in order to convey a deeper understanding of the purpose behind the mathematics. To conclude this chapter, \cref{sec:FIPhysics} explains how the Fisher Information can be used to find phase transitions in physical systems, to illustrate its wide range of applications.\\
\cref{sec:ChapterNTK} introduces the NTK, by going over its derivation in \cref{sec:NTKderivation} and explaining it in more detail in \cref{sec:NTKInterpretation}.\\
\cref{sec:ChapterResults} is divided into two sections. In \cref{sec:Results1}, we derive a relationship between the trace of the Fisher Information and the NTK and examine how accurately the behavior of their traces match up for a training example on the MNIST dataset. In \cref{sec:Results2} compute examples of the Fisher Information, the NTK Trace and the scalar curvature for a simple, 2-input neural network.\\
\cref{sec:ChapterConcAndOutlook} provides a summary of the mathematical concepts explained and investigated in this work. We also discuss potential further topics of research on the considered observables.